{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性模型\n",
    "这里是线性模型的Index,也就是目录.主要是为了简单介绍各类线性模型及其参数,为Python机器学习可视化编辑器做准备.\n",
    "\n",
    "### 普通最小二乘法\n",
    "**普通最小二乘回归:LinearRegression**\n",
    " - fit_intercept:默认为True,是否使用截距\n",
    " - normalize:bool,默认为False,是否对数据进行标准化(l2范数)\n",
    " - copy:是否复制x,默认为True,否则将覆盖\n",
    " - n_jobs:用于计算的作业上,默认None,-1表示所有"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "# y = 1 * x_0 + 2 * x_1 + 3\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "reg2 = LinearRegression().fit(X, y)\n",
    "reg2.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0000000000000018"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.predict(np.array([[3, 5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 岭回归和分类\n",
    "Ridge and RidgeClassifier\n",
    "**具有l2正则化的线性最小二乘法:l2**\n",
    " - alpha:正则化参数,默认为1,正则强度,必须为浮点数.\n",
    " - fit_intercept:是否使用解决,默认使用\n",
    " - normalize:是否进行标准化,默认不使用\n",
    " - copy_x:是否copy训练数据集,默认负责,否则将覆盖\n",
    " - max_iter:int数据类型，默认None；对sparse_cg”和“ lsqr”求解器，默认值由scipy.sparse.linalg确定。对于“sag”求解器，默认值为1000。\n",
    " - tol:求解方案的精度,默认1e-3\n",
    " - solver:求解器:{'auto'，'svd'，'cholesky'，'lsqr'，'sparse_cg'，'sag'，'saga'}，默认='auto',auto根据数据类型自动选择,svd,奇异值分解,对于奇异矩阵，比“ Cholesky”更稳定。'cholestsky'使用标准的scipy.linalg.solve函数获得封闭形式的解决方案。'sparse_cg'使用scipy.sparse.linalg.cg中的共轭梯度求解器。作为一种迭代算法，对于大规模数据（设置tol和的可能性max_iter），此求解器比“ Cholesky”更合适。“ lsqr”使用专用的正则化最小二乘例程scipy.sparse.linalg.lsqr。它是最快的，并且使用迭代过程。“ sag”使用随机平均梯度下降，“ saga”使用其改进的无偏版本SAGA。两种方法都使用迭代过程，并且当n_samples和n_features都较大时，通常比其他求解器更快。请注意，只有在比例大致相同的要素上才能保证“ sag”和“ saga”快速收敛。您可以使用sklearn.preprocessing中的缩放器对数据进行预处理。\n",
    " - random_state:sag或saga所需要的随机数\n",
    " - class_weight:dict or ‘balanced’, default=None;与类相关的权重,格式为{class_label：weight}。 如果未给出，则所有类都应具有权重一。“平衡”模式使用y的值自动将权重与输入数据中的类频率成反比地调整为n_samples /（n_classes * np.bincount（y））。回归算法没有该参数,分类算法特有参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6836781050289735"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "n_samples, n_features = 10, 5\n",
    "rng = np.random.RandomState(0)\n",
    "y = rng.randn(n_samples)\n",
    "X = rng.randn(n_samples, n_features)\n",
    "clf = Ridge(alpha=1.0)\n",
    "clf.fit(X, y)\n",
    "clf.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9595782073813708"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "clf = RidgeClassifier().fit(X, y)\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso(Least absolute shrinkage and selection operator)\n",
    "带有L1正则化的线性模型\n",
    "**Lasso**\n",
    " - alpha:与L1项相乘的常数,默认为1.0\n",
    " - fit_intercept:是否计算此模型的截距,默认为True\n",
    " - normalize:标准化,默认为False\n",
    " - precompute:预计算,auto\n",
    " - copy_x:是否复制x,否则将覆盖原数据,默认为True\n",
    " - max_iter:最大迭代次数,默认1000\n",
    " - tol:默认=1e-4.优化的容限：如果更新小于tol，则优化代码检查对偶间隙的最佳性，并继续进行直到其小于tol。\n",
    " - warm_start:默认False,设置为True时,请重用上一个调用的解决方案以适合初始化，否则，只需擦除以前的解决方案即可。\n",
    " - postive:默认False,是否强制系数为整数\n",
    " - random_state:随机数种子,默认为None\n",
    " - selection:范围{'cyclic'，'random'}，默认='cyclic'.如果设置为“随机”，则随机系数将在每次迭代时更新，而不是默认情况下按顺序遍历要素。这（设置为“随机”）通常会导致收敛更快，尤其是当tol高于1e-4时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "      normalize=False, positive=False, precompute=False, random_state=None,\n",
       "      selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.Lasso(alpha=0.1)\n",
    "clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85 0.  ]\n"
     ]
    }
   ],
   "source": [
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15000000000000002\n"
     ]
    }
   ],
   "source": [
    "print(clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet\n",
    "**将L1和L2先验组合作为正则化器的线性回归。ElasticNet**\n",
    " - alpha:默认为1,乘以惩罚项的常数\n",
    " - l1_ratio:混合参数,默认为0.5,详见原论文\n",
    " - fit_intercept:是否使用截距,默认为True.\n",
    " - normalize:是否标准化.默认为True,fit_intercept设置为False 时，将忽略此参数。如果为True，则在回归之前通过减去均值并除以l2-范数来对回归变量X进行归一化。\n",
    " - precompute:是否进行预计算,默认False\n",
    " - max_iter:迭代次数,默认1000\n",
    " - copy_X:是否复制数据,默认True\n",
    " - tol:默认=1e-4.优化的容限：如果更新小于tol，则优化代码检查对偶间隙的最佳性，并继续进行直到其小于tol。\n",
    " - warm_start:默认False,设置为True时,请重用上一个调用的解决方案以适合初始化，否则，只需擦除以前的解决方案即可。\n",
    " - postive:默认False,是否强制系数为整数\n",
    " - random_state:随机数种子,默认为None\n",
    " - selection:范围{'cyclic'，'random'}，默认='cyclic'.如果设置为“随机”，则随机系数将在每次迭代时更新，而不是默认情况下按顺序遍历要素。这（设置为“随机”）通常会导致收敛更快，尤其是当tol高于1e-4时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
       "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "           random_state=0, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_features=2, random_state=0)\n",
    "regr = ElasticNet(random_state=0)\n",
    "regr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18.83816048 64.55968825]\n"
     ]
    }
   ],
   "source": [
    "print(regr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4512607561654027\n"
     ]
    }
   ],
   "source": [
    "print(regr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.45126076]\n"
     ]
    }
   ],
   "source": [
    "print(regr.predict([[0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-task Elastic-Net\n",
    "### Least Angle Regression\n",
    "### LARS Lasso\n",
    "### Orthogonal Matching Pursuit (OMP)\n",
    "### Bayesian Regression\n",
    "### Logistic regression\n",
    "逻辑回归尽管有其名称，但它是用于分类而非回归的线性模型。 逻辑回归在文献中也称为对数回归，最大熵分类（MaxEnt）或对数线性分类器。 在此模型中，使用逻辑函数对描述单个试验可能结果的概率进行建模。\n",
    "\n",
    "**参数:**\n",
    "\n",
    " - penalty:{'l1'，'l2'，'elasticnet'，'none'}默认\"l2\".用于指定正则化器(或者说惩罚项)中使用的范数。'newton-cg'，'sag'和'lbfgs'求解器仅支持l2罚分。仅“ saga”求解器支持“ elasticnet”。如果为“ none”（liblinear求解器不支持），则不应用任何正则化。\n",
    " - dual:bool,默认Fasle,对偶或原始配方,对偶公式化仅使用liblinear求解器实现了12惩罚。当n_samples> n_features时，首选dual = False。\n",
    " - tol:停止标准的公差。默认1e-4\n",
    " - C:默认为1,正则强度的倒数；必须为正浮点数。与支持向量机一样，较小的值指定更强的正则化。\n",
    " - fit_intercept:默认为True,是否应将常量（aka偏置或截距）添加到决策函数。\n",
    " - intercept_scaling:默认为1,仅在使用求解器“ liblinear”并将self.fit_intercept设置为True时有用。在这种情况下，x变为[x，self.intercept_scaling]，即，将常量值等于intercept_scaling的“合成”特征附加到实例矢量。截距变为。intercept_scaling * synthetic_feature_weight.注意！与所有其他特征一样，合成特征权重也要经过l1 / l2正则化。为了减轻正则化对合成特征权重（以及因此对截距）的影响，必须增加intercept_scaling。\n",
    " - class_weight:dict或者balanced,默认为None\n",
    " - random_state:在求解器为\"sag\"或:liblinear\"时,用于随机整理数据.\n",
    " - solver:求解器,{'newton-cg'，'lbfgs'，'liblinear'，'sag'，'saga'}，默认='lbfgs'.用于优化问题的算法.对于小型数据集，“ liblinear”是一个不错的选择，而对于大型数据集，“ sag”和“ saga”则更快。对于多类问题，只有'newton-cg'，'sag'，'saga'和'lbfgs'处理多项式损失。“ liblinear”仅限于“一站式”计划。'newton-cg'，'lbfgs'，'sag'和'saga'处理L2或不罚分'liblinear'和'saga'也可以处理L1罚款“ saga”还支持“ elasticnet”惩罚'liblinear'不支持设置 penalty='none'请注意，只有在比例大致相同的要素上才能保证“ sag”和“ saga”快速收敛。您可以使用sklearn.preprocessing中的缩放器对数据进行预处理。'lbfgs'='liblinear'\n",
    " - max_iter:int,默认100;求解器收敛所学的最大迭代次数\n",
    " - multi_class:{'auto'，'ovr'，'multinomial'}，默认='auto'.如果选择的选项是“ovr”(=auto)，则每个标签都适合二进制问题。对于“多项式”，即使数据是二进制的，最小化损失也就是整个概率分布中的多项式损失拟合。当solver ='liblinear'时，'multinomial'不可用。如果数据是二进制的，或者如果Solver ='liblinear'，则'auto'选择'ovr'，否则选择'multinomial'。\n",
    " - verbose:当设置liblinear\"(=lbfgs)时,设置任意正数以表示详细程度,默认0\n",
    " - warm_start:默认False,设置为True时,请重用上一个调用的解决方案以适合初始化，否则，只需擦除以前的解决方案即可。\n",
    " - n_jobs:默认None,如果multi_class ='ovr'”，则在对类进行并行化时使用的CPU内核数。将solver设置为“ liblinear”时，无论是否指定“ multi_class” ，都将忽略此参数。-1表示使用所有处理器(会占用拖拽式图形编辑器的管理线程)\n",
    " - l1_ratio:Elastic-Net混合参数,默认None,不使用。Elastic-Net混合参数，0 <= l1_ratio <=1。仅当惩罚=“ elasticnet”时使用。 设置l1_ratio = 0等效于使用惩罚=“ l2”，而设置l1_ratio = 1等效于使用惩罚=“ l1”。 对于0 <l1_ratio <1，惩罚是L1和L2的组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fonttian/anaconda3/envs/keras/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "clf.predict(X[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.81793126e-01, 1.82068601e-02, 1.44258435e-08],\n",
       "       [9.71718481e-01, 2.82814892e-02, 3.01626771e-08]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(X[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 广义线性回归\n",
    "### 随机梯度下降\n",
    "**随机梯度下降分类器,SGDClassifier**\n",
    "具有SGD训练的线性分类器（SVM，逻辑回归等）。该估计器通过随机梯度下降（SGD）学习实现正则化线性模型：一次估计每个样本的损失梯度，并以递减的强度进度表（又称学习率）对模型进行更新。SGD允许通过该partial_fit方法进行小批量（在线/核心外）学习。为了使用默认学习率计划获得最佳结果，数据应具有零均值和单位方差。\n",
    "\n",
    "**参数：**\n",
    "\n",
    " - loss:默认-hinge,此时将给出线性SVM,可选项为:‘hinge’, ‘log’, ‘modified_huber’, ‘squared_hinge’, ‘perceptron’, or a regression loss: ‘squared_loss’, ‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’.对数损失使逻辑回归成为概率分类器。'modified_huber'是另一个平滑的损失，它使异常值和概率估计具有容忍度。“ squared_hinge”就像'hinge'一样，但会受到二次惩罚。“感知器”是感知器算法使用的线性损耗。其他损失是为回归而设计的，但也可用于分类。\n",
    " - penalty:{‘l2’, ‘l1’, ‘elasticnet’}, default=’l2’.要使用的惩罚（又称正则化术语）。默认值为“ l2”，这是线性SVM模型的标准正则化器。“ l1”和“ elasticnet”可能会给模型带来稀疏性（特征选择）,这一点“ l2”是无法实现的。\n",
    " - alpha:浮点数,默认为0.0001,与正则化项城的常数,值越高,正则化越强.当learning_rate设置为“optimal” 时，也用于计算学习率。\n",
    " - l1-ratio:0 <= l1_ratio <= 1的Elastic Net混合参数。l1_ratio = 0对应于L2惩罚，l1_ratio = 1对应L1。 仅在惩罚为“ elasticnet”时使用。\n",
    " - fit_intercept:默认为True,是否使用截距\n",
    " - max_iter:通过训练数据的的最大次数,默认1000\n",
    " - tol:停止标准,如果不是None,则n_iter_no_change连续（eploss> best_loss-tol）时训练将停止。默认1e-3\n",
    " - shuffle:默认为True,在每个时期之后是否应重新整理训练数据\n",
    " - verbose:默认0,详细程度\n",
    " - epsilon:默认0.1 ,ε对ε不敏感的损失函数；仅当loss是“ huber”，“ epsilon_insensitive”或“ squared_epsilon_insensitive”时。对于“休伯”，确定使预测正确正确变得不那么重要的阈值。对于不敏感epsilon的情况，如果当前预测和正确标签之间的任何差异小于此阈值，则将忽略它们。\n",
    " - n_jobs:默认None,用于执行OVA的CPU数量,-1表示全部,会对图形编辑器的管理线程造成资源挤兑\n",
    " - random_state:用于shuffle的随机数种子\n",
    " - learning_state:学习率,默认\"optimal\".'constant'： eta = eta0;“optimal”： 其中t0由Leon Bottou提出的启发式方法选择。eta = 1.0 / (alpha * (t + t0)),'invscaling'： eta = eta0 / pow(t, power_t),'adaptive'：eta = eta0，只要训练持续减少即可。每次n_iter_no_change个连续的纪元未能将训练损失减少一倍，或者如果early_stopping为True，则未能增加鉴定分数一次，则将当前学习率除以5。\n",
    " - eta0:double类型,默认为0;“固定(constant)”，“增量(invscaling)”或“自适应(adaptive)”进度表的初始学习率。默认值为0.0，因为默认计划“最佳(optimal)”未使用eta0。\n",
    " - power_t:double,反比例学习率的指数[默认0.5]。\n",
    " - early_stopping:默认False;当验证分数没有提高时是否使用提前停止来终止训练。如果设置为True，它将自动将训练数据的分层部分score留作验证，并在该方法返回的验证分数没有连续n_iter_no_change个时期至少提高tol 时终止训练。\n",
    " - validation_fraction:默认0.1,预留的训练数据比例作为早期停止的验证集。必须介于0和1之间。仅在early_stopping为True时使用。\n",
    " - n_iter_no_change:int,默认为5,迭代次数;没有改善时需要提前停止(早停,early_stopping)超参数。\n",
    " - class_weight:默认None,可选-dict或者balanced,与类关联的权重。如果未给出，则所有类均应具有权重一。“平衡”模式使用y的值来自动调整与输入数据中的类频率成反比的权重。n_samples / (n_classes * np.bincount(y))\n",
    " - warm_start:默认False,设置为True时，请重用上一个调用的解决方案以适合初始化，否则，只需擦除以前的解决方案即可。当warm_start为True时，重复调用fit或partial_fit可能会导致解决方案与一次调用fit时有所不同，这是因为数据的重排方式所致。如果使用动态学习率，则根据已经看到的样本数调整学习率。调用会fit重置此计数器，而partial_fit会导致增加现有计数器。\n",
    " - average:默认False,设置为True时，将计算所有更新的平均SGD权重并将结果存储在coef_属性中。如果将int设置为大于1，则一旦看到的样本总数达到，就会开始平均average。因此，average=10将在看到10个样本后开始平均。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
    "Y = np.array([1, 1, 2, 2])\n",
    "# Always scale the input. The most convenient way is to use a pipeline.\n",
    "clf = make_pipeline(StandardScaler(),\n",
    "                    SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "clf.fit(X, Y)\n",
    "\n",
    "\n",
    "print(clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**随机梯度下降回归,SGDRegressor**\n",
    "\n",
    "通过使用SGD最小化正则经验损失来拟合线性模型。SGD代表随机梯度下降：每次估计每个样本的损失梯度，并随着强度进度表（即学习率）的降低而更新模型。正则化器是对损失函数的一种惩罚，它使用平方的欧几里德范数L2或绝对范数L1或两者的组合（弹性网）将模型参数向零矢量收缩。如果由于正则化而使参数更新超过0.0值，则更新将被截断为0.0以允许学习稀疏模型并实现在线特征选择。\n",
    "\n",
    " - loss:默认-squared_loss;可选项为:squared_loss’, ‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’;“ squared_loss”是指普通的最小二乘拟合。'huber'修改了'squared_loss'，通过将平方损失转换为线性损失超过ε距离，从而减少了对异常值校正的关注。'epsilon_insensitive'忽略小于epsilon的错误，并且超出该范围呈线性关系；这是SVR中使用的损失函数。'squared_epsilon_insensitive'是相同的，但是变成超过ε容差的平方损耗。\n",
    " - penalty:{‘l2’, ‘l1’, ‘elasticnet’}, default=’l2’.要使用的惩罚（又称正则化术语）。默认值为“ l2”，这是线性SVM模型的标准正则化器。“ l1”和“ elasticnet”可能会给模型带来稀疏性（特征选择）,这一点“ l2”是无法实现的。\n",
    " - alpha:浮点数,默认为0.0001,与正则化项城的常数,值越高,正则化越强.当learning_rate设置为“optimal” 时，也用于计算学习率。\n",
    " - l1-ratio:0 <= l1_ratio <= 1的Elastic Net混合参数。l1_ratio = 0对应于L2惩罚，l1_ratio = 1对应L1。 仅在惩罚为“ elasticnet”时使用。\n",
    " - fit_intercept:默认为True,是否使用截距\n",
    " - max_iter:通过训练数据的的最大次数,默认1000\n",
    " - tol:停止标准,如果不是None,则n_iter_no_change连续（eploss> best_loss-tol）时训练将停止。,默认1e-3\n",
    " - shuffle:默认为True,在每个时期之后是否应重新整理训练数据\n",
    " - verbose:默认0,详细程度\n",
    " - epsilon:默认0.1 ,ε对ε不敏感的损失函数；仅当loss是“ huber”，“ epsilon_insensitive”或“ squared_epsilon_insensitive”时。对于“休伯”，确定使预测正确正确变得不那么重要的阈值。对于不敏感epsilon的情况，如果当前预测和正确标签之间的任何差异小于此阈值，则将忽略它们。\n",
    " - n_jobs:默认None,用于执行OVA的CPU数量,-1表示全部,会对图形编辑器的管理线程造成资源挤兑\n",
    " - random_state:用于shuffle的随机数种子\n",
    " - learning_state:学习率,默认\"invscaling\";'invscaling'： eta = eta0 / pow(t, power_t);'constant'： eta = eta0;“optimal”： 其中t0由Leon Bottou提出的启发式方法选择。eta = 1.0 / (alpha * (t + t0)),'invscaling'： eta = eta0 / pow(t, power_t),'adaptive'：eta = eta0，只要训练持续减少即可。每次n_iter_no_change个连续的纪元未能将训练损失减少一倍，或者如果early_stopping为True，则未能增加鉴定分数一次，则将当前学习率除以5。\n",
    " - eta0:默认0.01;“固定”，“增量”或“自适应”进度表的初始学习率。默认值为0.0，因为默认计划“最佳”未使用eta0。\n",
    " - power_t:反比例学习率的指数[默认0.25]。\n",
    " - early_stopping:默认False;当验证分数没有提高时是否使用提前停止来终止训练。如果设置为True，它将自动将训练数据的分层部分score留作验证，并在该方法返回的验证分数没有连续n_iter_no_change个时期至少提高tol 时终止训练。\n",
    " - validation_fraction:默认0.1,预留的训练数据比例作为早期停止的验证集。必须介于0和1之间。仅在early_stopping为True时使用。\n",
    " - n_iter_no_change:默认,迭代次数，没有改善，需要提前停止。\n",
    " - class_weight:默认None,可选-dict或者balanced,与类关联的权重。如果未给出，则所有类均应具有权重一。“平衡”模式使用y的值来自动调整与输入数据中的类频率成反比的权重。n_samples / (n_classes * np.bincount(y))\n",
    " - warm_start:默认False,设置为True时，请重用上一个调用的解决方案以适合初始化，否则，只需擦除以前的解决方案即可。当warm_start为True时，重复调用fit或partial_fit可能会导致解决方案与一次调用fit时有所不同，这是因为数据的重排方式所致。如果使用动态学习率，则根据已经看到的样本数调整学习率。调用会fit重置此计数器，而partial_fit会导致增加现有计数器。\n",
    " - average:默认False,设置为True时，将计算所有更新的平均SGD权重并将结果存储在coef_属性中。如果将int设置为大于1，则一旦看到的样本总数达到，就会开始平均average。因此，average=10将在看到10个样本后开始平均。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardscaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('sgdregressor',\n",
       "                 SGDRegressor(alpha=0.0001, average=False, early_stopping=False,\n",
       "                              epsilon=0.1, eta0=0.01, fit_intercept=True,\n",
       "                              l1_ratio=0.15, learning_rate='invscaling',\n",
       "                              loss='squared_loss', max_iter=1000,\n",
       "                              n_iter_no_change=5, penalty='l2', power_t=0.25,\n",
       "                              random_state=None, shuffle=True, tol=0.001,\n",
       "                              validation_fraction=0.1, verbose=0,\n",
       "                              warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "n_samples, n_features = 10, 5\n",
    "rng = np.random.RandomState(0)\n",
    "y = rng.randn(n_samples)\n",
    "X = rng.randn(n_samples, n_features)\n",
    "# Always scale the input. The most convenient way is to use a pipeline.\n",
    "reg = make_pipeline(StandardScaler(),\n",
    "                    SGDRegressor(max_iter=1000, tol=1e-3))\n",
    "reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知器\n",
    "### 被动攻击算法\n",
    "### 稳健性回归:离群值和建模错误\n",
    "### 多项式回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
