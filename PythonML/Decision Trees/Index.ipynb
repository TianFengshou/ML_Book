{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树\n",
    "决策树（DT）是一种用于分类和回归的非参数监督学习方法。目标是创建一个模型，该模型通过学习从数据特征推断出的简单决策规则来预测目标变量的值。\n",
    "\n",
    "### 分类决策树\n",
    "\n",
    "**参数:** \n",
    " - criterion{“gini”, “entropy”}, 默认为”gini”;分割判断标准,基尼或者信息熵\n",
    " \n",
    " - splitter{“best”, “random”}, 默认为”best”;分割策略,最佳或者随机\n",
    " \n",
    " - max_depth:int, 默认为None;树的最大深度,直到所有叶子都是纯净的,或者直到所有叶子都包含至少min_samples_split个样本。\n",
    " \n",
    " - min_samples_split:int or float, 默认为2;拆分内部节点所需的最少样本数,如果为整数则表示最小样本数,如果为float则表示百分比,ceil(min_samples_split * n_samples)\n",
    " \n",
    " - min_samples_leaf:int or float, 默认为1;在叶节点处需要的最小样本数。仅在任何深度的分裂点在min_samples_leaf左分支和右分支中的每个分支上至少留下训练样本时，才考虑。这可能具有平滑模型的效果，尤其是在回归中。如果为整数则表示最小样本数,如果为float则表示百分比ceil(min_samples_leaf*min_samples_leaf)\n",
    " \n",
    " - min_weight_fraction_leaf:float, 默认为0.0;在所有叶节点处（所有输入样本）的权重总和中的最小加权分数。如果未提供sample_weight，则样本的权重相等。\n",
    " \n",
    " - max_features:int, float or {“auto”, “sqrt”, “log2”}, 默认为None;寻找最佳分割时要考虑的功能数量：如果为int，则max_features在每个拆分处考虑要素。如果为float，max_features则为小数，并 在每次拆分时考虑要素。int(max_features * n_features)。如果“自动”，则max_features=sqrt(n_features)。如果是“ sqrt”，则max_features=sqrt(n_features)。如果为“ log2”，则为max_features=log2(n_features)。如果没有，则max_features=n_features。注意：直到找到至少一个有效的节点样本分区，分割的搜索才会停止，即使它需要有效检查多个max_features要素也是如此。\n",
    " \n",
    " - random_state:int, RandomState instance, 默认为None;随机数种子\n",
    " \n",
    " - max_leaf_nodes:int, 默认为None;以最佳的方式建立树。最佳节点定义为杂质的相对减少。如果为None，则叶节点数不受限制。\n",
    " \n",
    " - min_impurity_decrease:float, 默认为0.0;如果节点分裂导致杂质减少大于或等于该值，则该节点将分裂。\n",
    " \n",
    " - min_impurity_split：float, 默认为0;树木生长尽早停止的阈值。如果节点的杂质高于阈值，则该节点将分裂，否则为叶。将于0.25删除\n",
    " \n",
    " - class_weight:dict, list of dict or “balanced”, 默认为None。与形式的类有关的权重。“平衡”模式使用y的值来自动调整与输入数据中的类频率成反比的权重，如下所示： n_samples / (n_classes * np.bincount(y))。对于多输出，y的每一列的权重将相乘。请注意，如果指定了sample_weight，则这些权重将与sample_weight（通过fit方法传递）相乘。\n",
    " \n",
    " - presort:deprecated, 默认为’deprecated’;弃用，并将在v0.24删除\n",
    " \n",
    " - ccp_alpha:non-negative float, 默认为0.0;用于最小化成本复杂性修剪的复杂性参数。具有最大成本复杂度的子树小于ccp_alpha所选择的子树 。默认情况下，不执行修剪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.33333333, 0.33333333, 0.33333333, 0.33333333,\n",
       "       0.33333333, 0.33333333, 0.33333333, 0.33333333, 0.33333333])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(max_depth=3.5,random_state=0,ccp_alpha=0.5)\n",
    "iris = load_iris()\n",
    "cross_val_score(clf, iris.data, iris.target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回归决策树\n",
    "**参数:** \n",
    " - criterion：{“mse”, “friedman_mse”, “mae”}, default=”mse”，分割时的衡量标准。\n",
    " - splitter:{“best”, “random”}, 默认为”best”;分割策略,最佳或者随机。\n",
    " - max_depth:int, 默认为None;树的最大深度,知道所有叶子都是纯净的,或者直到所有叶子都包含至少min_samples_split个样本。\n",
    " - min_samples_split:int or float, 默认为2;拆分内部节点所需的最少样本数,如果为整数则表示最小样本数,如果为float则表示百分比,ceil(min_samples_split * n_samples)。\n",
    " - min_samples_leaf:int or float, 默认为1;在叶节点处需要的最小样本数。仅在任何深度的分裂点在min_samples_leaf左分支和右分支中的每个分支上至少留下训练样本时，才考虑。这可能具有平滑模型的效果，尤其是在回归中。如果为整数则表示最小样本数,如果为float则表示百分比ceil(min_samples_leaf*min_samples_leaf)。\n",
    " - min_weight_fraction_leaf:float, 默认为0.0;在所有叶节点处（所有输入样本）的权重总和中的最小加权分数。如果未提供sample_weight，则样本的权重相等。\n",
    " - max_features:int, float or {“auto”, “sqrt”, “log2”}, 默认为None;寻找最佳分割时要考虑的功能数量：如果为int，则max_features在每个拆分处考虑要素。如果为float，max_features则为小数，并 在每次拆分时考虑要素。int(max_features * n_features)。如果“自动”，则max_features=sqrt(n_features)。如果是“ sqrt”，则max_features=sqrt(n_features)。如果为“ log2”，则为max_features=log2(n_features)。如果没有，则max_features=n_features。注意：直到找到至少一个有效的节点样本分区，分割的搜索才会停止，即使它需要有效检查多个max_features要素也是如此。\n",
    " - random_state:int, RandomState instance, 默认为None;随机数种子。\n",
    " - max_leaf_nodes:int, 默认为None;以最佳的方式建立树。最佳节点定义为杂质的相对减少。如果为None，则叶节点数不受限制。\n",
    " - min_impurity_decrease:float, 默认为0.0;如果节点分裂导致杂质减少大于或等于该值，则该节点将分裂。\n",
    " - min_impurity_split：float, 默认为0;树木生长尽早停止的阈值。如果节点的杂质高于阈值，则该节点将分裂，否则为叶。将于0.25删除。\n",
    " - presort:deprecated, 默认为’deprecated’;弃用，并将在v0.24删除。\n",
    " - ccp_alpha:non-negative float, 默认为0.0;用于最小化成本复杂性修剪的复杂性参数。具有最大成本复杂度的子树小于ccp_alpha所选择的子树 。默认情况下，不执行修剪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.39292219, -0.46749346,  0.02768473,  0.06441362, -0.50323135,\n",
       "        0.16437202,  0.11242982, -0.73798979, -0.30953155, -0.00137327])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "regressor = DecisionTreeRegressor(random_state=0)\n",
    "cross_val_score(regressor, X, y, cv=10)\n",
    "                   # doctest: +SKIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
